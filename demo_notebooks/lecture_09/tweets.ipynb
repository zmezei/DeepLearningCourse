{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,LSTM,TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal in this notebook is to create a model which is able to generate tweets.  Tweets are small messages with maximum length of 280 character. We will build a character level model to be able to achieve this goal.\n",
    "\n",
    "Tweets can contain simleys, hashtags (which starts with #), urls and user citation (starting with @). First of all we need to prepocess the tweets. \n",
    "\n",
    "A few important prepocessing steps:\n",
    "\n",
    "1. Transform the tweets to lowercase string\n",
    "\n",
    "2. Transfrom \"#something\" to \"&lt;hashtag&gt; something\"\n",
    "\n",
    "3. Transform \"@USER\" to \"USER &lt;user&gt;\"\n",
    "\n",
    "4. Transform \"DNC\" to \"dnc &lt;allcaps&gt;\"\n",
    "\n",
    "5. Transform urls: \"http://google.com\" to \"&lt;url&gt;\"\n",
    "\n",
    "6. Add \"&lt;end&gt;\" to end of every tweet\n",
    "\n",
    "7. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will preporcess a single tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body)\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + [hashtag_body.lower()])\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "def user(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \"<user>\"\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", user)\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    text = re_sub(r\"(\\s|\\uFEFF|\\xA0)+\",\" \")\n",
    "\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To learn to generate tweets we have to download a lot of tweets. To make the example more interesting, lets say we want to generate tweets in Dondald Trump style. \n",
    "\n",
    "First I downloaded almost all of Trump's tweets (around 33k tweets) from (http://www.trumptwitterarchive.com/about), and I saved to \"trump.txt\" file. In the file every line is a tweet.\n",
    "\n",
    "First we need to find out what kind of characters are used. So we go trough the tweets, we tokenize every tweet and then we count the occurence of the characters. The special characters from preprocessing \"&lt;somehing&gt;\" will be one special character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"trump.txt\",\"r\")\n",
    "\n",
    "characters = set()\n",
    "counter = {}\n",
    "lengths = []\n",
    "for line in f:\n",
    "    lengths.append(len(line))\n",
    "    lineiter = iter(tokenize(line))\n",
    "    for char in lineiter:\n",
    "        if char==\"<\":\n",
    "            spec = \"<\"\n",
    "            while char!=\">\":\n",
    "                char = next(lineiter)\n",
    "                spec += char\n",
    "            characters.add(spec)\n",
    "            if spec in counter:\n",
    "                counter[spec] += 1\n",
    "            else:\n",
    "                counter[spec] = 1\n",
    "        else:\n",
    "            characters.add(char)\n",
    "            if char in counter:\n",
    "                counter[char] +=1\n",
    "            else:\n",
    "                counter[char] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```counter``` dictionary will contain every character and it's occurrence in Donald Trump's tweets. \n",
    "\n",
    "We want to create a vocabularry: containing all the characters in a list, where the position will code the character.  There are a lot of special characters (e.g. Japanese symbols, smileys) we don't really want and they are pretty rare. So we remove all the characters which appeard less then 30 times in the tweets. We also add two new character \"&lt;unknown&gt;\" for unknown characters and \"&lt;end&gt;\" to signal the end of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "char_to_idx = {}\n",
    "i = 2\n",
    "for c in counter:\n",
    "    if counter[c]>30:\n",
    "        char_to_idx[c] = i\n",
    "        i += 1\n",
    "char_to_idx[\"<unknown>\"] = 1\n",
    "char_to_idx[\"<end>\"] = i\n",
    "\n",
    "idx_to_char = ['' for i in range(71)]\n",
    "for c in char_to_idx:\n",
    "    idx_to_char[char_to_idx[c]] = c\n",
    "\n",
    "print(len(char_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"vocabulary\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 25, 'p': 44, 'q': 26, '<unknown>': 1, '.': 21, 'e': 45, 'y': 28, 'z': 27, '#': 49, '‚Äú': 2, 'üëç': 47, 'c': 58, '‚Äî': 3, '*': 4, \"'\": 29, '‚Ä¶': 60, '+': 5, '<user>': 54, '<end>': 70, '<url>': 63, '‚Äô': 61, 'v': 62, 'i': 6, 'f': 31, 'Ô∏è': 56, ')': 32, '‚Äò': 7, '-': 8, 'üá∏': 48, 'l': 9, 'o': 10, '(': 64, '!': 12, 'u': 46, '&': 33, 'n': 24, 'r': 51, '@': 13, '~': 53, ' ': 34, 'g': 30, ';': 66, 'd': 14, 's': 15, 't': 35, 'w': 36, '‚û°': 65, 'x': 37, 'üá∫': 50, '‚Äì': 11, '<elong>': 55, '\"': 16, '/': 52, '=': 17, '<smile>': 18, '<allcaps>': 38, 'h': 19, '$': 67, '‚Äù': 39, 'm': 68, '_': 40, 'j': 69, ':': 57, 'b': 41, '<hashtag>': 59, '?': 42, '<number>': 20, 'k': 22, '<repeat>': 23, '%': 43}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"character_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump([char_to_idx, idx_to_char], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vocabulary we transform every tokenized tweet to a list of indexes. We create to lists: X, Y (input, output). The output is simply shifted with one character:\n",
    "\n",
    "<img src=\"imgs/tweet_shift.PNG\" width=\"60%\" />\n",
    "\n",
    "We also transform the list of indexes to one-hot representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"trump.txt\",\"r\")\n",
    "\n",
    "max_len = max(lengths)\n",
    "\n",
    "tweets = []\n",
    "X = []\n",
    "Y = []\n",
    "for line in f:\n",
    "    lineiter = iter(tokenize(line))\n",
    "    tweet = []\n",
    "    \n",
    "    for char in lineiter:\n",
    "        if char==\"<\":\n",
    "            spec = \"<\"\n",
    "            while char!=\">\":\n",
    "                char = next(lineiter)\n",
    "                spec += char\n",
    "            if spec in char_to_idx:\n",
    "                tweet.append(char_to_idx[spec])\n",
    "            else:\n",
    "                tweet.append(char_to_idx[\"<unknown>\"])\n",
    "        else:\n",
    "            if char in char_to_idx:\n",
    "                tweet.append(char_to_idx[char])\n",
    "            else:\n",
    "                tweet.append(char_to_idx[\"<unknown>\"])\n",
    "    tweet.append(char_to_idx[\"<end>\"])\n",
    "    lt = len(tweet)\n",
    "    tweet.extend([0]*(max_len-lt))\n",
    "    tweet2 = copy.deepcopy(tweet)\n",
    "    tweet2.pop(0)\n",
    "    tweet.pop(-1)\n",
    "    tweets.append(tweet)\n",
    "    X.append(to_categorical(np.array(tweet),num_classes=71).reshape((1,-1,71)))\n",
    "    Y.append(to_categorical(np.array(tweet2),num_classes=71).reshape((1,-1,71)))\n",
    "\n",
    "tweets = np.array(tweets)\n",
    "X = np.vstack(X)\n",
    "Y = np.vstack(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33402, 314, 71)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "<img src=\"imgs/tweet_gen.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(X.shape[-1])))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 314, 300)          446400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 314, 300)          721200    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 314, 71)           21371     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 314, 71)           0         \n",
      "=================================================================\n",
      "Total params: 1,188,971\n",
      "Trainable params: 1,188,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "33402/33402 [==============================] - 84s 3ms/step - loss: 1.5423\n",
      "Epoch 2/200\n",
      "33402/33402 [==============================] - 74s 2ms/step - loss: 1.1145\n",
      "Epoch 3/200\n",
      "33402/33402 [==============================] - 74s 2ms/step - loss: 1.0863\n",
      "Epoch 4/200\n",
      "14336/33402 [===========>..................] - ETA: 41s - loss: 1.0539"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "        \"./trump_weights/weights-{epoch:02d}-{loss:.4f}.hdf5\",\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    ")\n",
    "\n",
    "model.fit(X,Y, batch_size=512, epochs=200, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"tweetgen_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating tweets\n",
    "\n",
    "<img src=\"imgs/tweet_gn.PNG\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = to_categorical(char_to_idx[\"c\"],num_classes=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.repeat(start, 314, axis=0).reshape((1,314,71))\n",
    "\n",
    "for i in range(1, 314):\n",
    "    y     = model.predict(seq)\n",
    "    new_c = np.random.choice(71, 1, p=y[0,i-1,:])[0]\n",
    "    seq[0,i,:] = to_categorical(new_c, num_classes=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=\"\"\n",
    "idxs = np.argmax(seq,axis=2).reshape((314))\n",
    "for idx in idxs:\n",
    "    tweet += idx_to_char[idx]\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtweets:\n",
    "    gtweets.append(tweet)\n",
    "else:\n",
    "    gtweets = [tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my unemployment rate ogothing \"crazy cuts (institedey) <url> <end>',\n",
       " 'my unemployment rate ogothing \"crazy cuts (institedey) <url> <end>',\n",
       " 'ashirt and obamacare will be attemeded to i told the evidence of dangers &amp; offers - atric\\' trump: \"a visit tomorrow morning‚Ä¶ <url>\" <end>',\n",
       " 'a bet <hashtag> votetrump natoo <allcaps>!#statbartinotay <hashtag> iacaucus <hashtag> commancerohebyand<url> <url> <end>',\n",
       " \"believe illegal immigration raised against obama's facts reso citizenship sunday! will be a loser! <end>\",\n",
       " 'crooked hillary we would be hosta chance to share a trale base with the election. just saw! <hashtag> alsocullardford <end>',\n",
       " 'collusion is brought with territors memoral to our military were fast. where is our country! she will snore to represent me! <end>',\n",
       " 'only endorsement clais scottish college poor presidential remember this show. <end>',\n",
       " 'crooked hillary wants terrible cyber borders and albanna. failed crowd will becoming in not great again! <end>',\n",
       " 'crooked hillary wants terrible cyber borders and albanna. failed crowd will becoming in not great again! <end>',\n",
       " 'crooked hillary who will stop treated are amazing. <hashtag> scotland <url> <end>',\n",
       " 'china get out and vote <allcaps> road to extance a sit de waste lifeline. marco ruincing manyants. <end>',\n",
       " 'somewhat global rolls interview will close the sanctions on tv <allcaps>. trump whica <allcaps> as president! <hashtag> trumppets <hashtag> trumpwaich <url> <end>',\n",
       " 'somewhat global rolls interview will close the sanctions on tv <allcaps>. trump whica <allcaps> as president! <hashtag> trumppets <hashtag> trumpwaich <url> <end>']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "The tweet tokenizer is based on <a href=\"https://gist.github.com/tokestermw/cb87a97113da12acb388\">python version</a>  of Ruby script to preprocess tweets for use in <a href=\"http://nlp.stanford.edu/projects/glove/\">GloVe</a> featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
