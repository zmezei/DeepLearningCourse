{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qati/.anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The dataset contains piano pieces mostly from Final Fantasy (<a href=\"https://github.com/Skuldur/Classical-Piano-Composer\">source</a>) in midi format.\n",
    "\n",
    "We use music21 to extract the data from midi files and also for create midi files.\n",
    "\n",
    "When we create a music21 object from a song it will be built with two objects: <b>Note</b>, <b>Chord</b>.\n",
    "    \n",
    "    \n",
    "### Note\n",
    "Contains information about:\n",
    "* <b>pitch</b>:  the frequency of the sound, represented with a leter ([A, B, C, D, E, F, G], high->low)\n",
    "* <b>octave</b>: which set of pitches you use on piano\n",
    "* <b>offset</b>: location of the note in the piece\n",
    "\n",
    "### Chord\n",
    "\n",
    "A set of notes to be played together.\n",
    "\n",
    "The dataset consist of <b>358</b> different notes and chords. For simplicity we fix the offset to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ```converter.parse``` we can create a music21 object from the mid file. After that we split the object to Notes and Chords. The chords will be represented as string, where the \".\" symbol separate the different notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "\n",
    "for file in glob.glob(\"./midi_songs/*.mid\"):\n",
    "    midi = converter.parse(file)\n",
    "\n",
    "    notes_to_parse = None\n",
    "    try:\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "    except:\n",
    "        pass\n",
    "    if parts:\n",
    "        notes_to_parse = parts.parts[0].recurse()\n",
    "    else: \n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, note.Note):\n",
    "            notes.append(str(element.pitch))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we read and processed all the mid files we will have a long list of notes and chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F4', 'F2', 'F4', 'F2', 'F4', 'F2', 'G#4', 'G#2', 'F4', 'F2', 'F4', 'F2', 'G#4', 'G#2', 'F4']\n",
      "['8.0', '7.11', 'G2', '6.10', '7.11', 'G2', '8.0', '3.8', '0.3', '3.8', '7.11', 'G2', '7.11', '1.6', '10.1', '1.6', '7.11', 'G2', '0.3', 'C2', '10.2', 'D2', 'E-2', '8.0', '7.11']\n"
     ]
    }
   ],
   "source": [
    "print(notes[0:15])\n",
    "print(notes[275:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57359\n"
     ]
    }
   ],
   "source": [
    "print(len(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different strings we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n"
     ]
    }
   ],
   "source": [
    "pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "print(len(pitchnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0.1', '0.1.5', '0.1.6', '0.2', '0.2.3.7', '0.2.4.7', '0.2.5', '0.2.6', '0.2.7', '0.3', '0.3.5', '0.3.5.8', '0.3.6', '0.3.6.8', '0.3.6.9', '0.3.7', '0.4', '0.4.5', '0.4.6', '0.4.7', '0.5', '0.5.6', '0.6', '1', '1.2', '1.2.4.6.8.10', '1.2.6', '1.2.6.8', '1.3', '1.3.5', '1.3.5.8', '1.3.6', '1.3.7', '1.3.8', '1.4', '1.4.6', '1.4.6.9', '1.4.7', '1.4.7.10', '1.4.7.9', '1.4.8', '1.5', '1.5.8', '1.5.9', '1.6', '1.7', '10', '10.0', '10.0.2.5', '10.0.3', '10.0.4', '10.0.5', '10.1', '10.1.3', '10.1.3.5.6', '10.1.3.6', '10.1.4', '10.1.4.6', '10.1.5', '10.11', '10.11.3', '10.11.3.5', '10.2', '10.2.3', '10.2.4', '10.2.5', '10.3', '11', '11.0', '11.0.4', '11.0.4.6', '11.0.4.7', '11.0.5', '11.1', '11.1.4', '11.1.4.5', '11.1.5', '11.1.6', '11.2', '11.2.4', '11.2.4.6', '11.2.4.7', '11.2.5', '11.2.5.7', '11.2.6', '11.3', '11.3.5', '11.3.6', '11.4', '11.4.5', '2', '2.3', '2.3.7', '2.3.7.10', '2.3.7.9', '2.4', '2.4.5', '2.4.5.9', '2.4.6.9.11', '2.4.7', '2.4.7.10', '2.4.8', '2.4.9', '2.5', '2.5.7', '2.5.7.10', '2.5.8', '2.5.8.10', '2.5.8.11', '2.5.9', '2.6', '2.6.10', '2.6.7', '2.6.7.9', '2.6.9', '2.7', '2.7.8', '2.8', '3', '3.4', '3.4.8', '3.4.8.10', '3.5', '3.5.10', '3.5.7', '3.5.7.10', '3.5.7.8.11', '3.5.8', '3.5.8.11', '3.6', '3.6.10', '3.6.7', '3.6.8', '3.6.9.11', '3.7', '3.7.10', '3.7.11', '3.7.9', '3.7.9.10', '3.8', '3.8.9', '3.9', '4', '4.10', '4.5', '4.5.7.11', '4.5.9', '4.5.9.0', '4.5.9.11', '4.6', '4.6.10', '4.6.11', '4.6.7', '4.6.8.11.1', '4.6.9', '4.6.9.11', '4.7', '4.7.10', '4.7.11', '4.7.9', '4.7.9.0', '4.7.9.11', '4.8', '4.8.10', '4.8.11', '4.8.9', '4.8.9.11', '4.9', '4.9.10', '5', '5.10', '5.10.11', '5.11', '5.6', '5.7', '5.7.0', '5.7.10', '5.7.8.0', '5.8', '5.8.0', '5.8.10', '5.8.11', '5.9', '5.9.0', '5.9.11', '5.9.11.0', '6', '6.10', '6.10.0', '6.10.1', '6.10.11', '6.10.11.1', '6.11', '6.11.0', '6.7', '6.7.0', '6.7.11', '6.7.11.2', '6.8', '6.8.0', '6.8.1', '6.8.10', '6.8.10.0.3', '6.8.11', '6.8.11.2', '6.9', '6.9.0', '6.9.0.2', '6.9.1', '6.9.11', '6.9.11.2', '7', '7.0', '7.10', '7.10.0', '7.10.0.3', '7.10.1', '7.10.1.3', '7.10.2', '7.11', '7.11.0', '7.11.2', '7.8', '7.8.0', '7.8.0.3', '7.8.10.2', '7.8.11', '7.9', '7.9.0', '7.9.1', '7.9.11', '7.9.11.2', '7.9.2', '8', '8.0', '8.0.1', '8.0.2', '8.0.3', '8.1', '8.10', '8.10.0', '8.10.1', '8.10.11.1.4', '8.10.11.3', '8.10.2', '8.10.3', '8.11', '8.11.1', '8.11.1.4', '8.11.2', '8.11.3', '8.9', '8.9.1', '8.9.1.3', '8.9.1.4', '8.9.11.1', '8.9.2', '9', '9.0', '9.0.2', '9.0.2.4', '9.0.2.5', '9.0.3', '9.0.3.5', '9.0.4', '9.1', '9.1.4', '9.10', '9.10.2', '9.10.2.4', '9.11', '9.11.0.4', '9.11.1', '9.11.2', '9.11.2.4', '9.11.2.5', '9.11.3', '9.11.4', '9.2', '9.2.3', 'A0', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'B-1', 'B-2', 'B-3', 'B-4', 'B-5', 'B-6', 'B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'C#1', 'C#2', 'C#3', 'C#4', 'C#5', 'C#6', 'C#7', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'E-1', 'E-2', 'E-3', 'E-4', 'E-5', 'E-6', 'E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'F#1', 'F#2', 'F#3', 'F#4', 'F#5', 'F#6', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'G#1', 'G#2', 'G#3', 'G#4', 'G#5', 'G#6', 'G1', 'G2', 'G3', 'G4', 'G5', 'G6']\n"
     ]
    }
   ],
   "source": [
    "print(pitchnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 358 different categories, we map each of them to an integer using a vocabulary (just like if they are words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(pitchnames)\n",
    "\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create input sequences of 100 length and we input it to a network, and based on this 100 notes and chords we want to predict the next element of the sequence. \n",
    "\n",
    "We use one-hot encoding for the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "\n",
    "network_input = []\n",
    "network_output = []\n",
    "\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "    sequence_in  = notes[i:i + sequence_length]\n",
    "    sequence_out = notes[i + sequence_length]\n",
    "    network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "n_patterns = len(network_input)\n",
    "\n",
    "network_input  = np.reshape(network_input, (-1, sequence_length, 1)) / float(n_vocab)\n",
    "network_output = np_utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "<img src=\"imgs/music_gen.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_37 (LSTM)               (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 358)               92006     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 358)               0         \n",
      "=================================================================\n",
      "Total params: 5,474,406\n",
      "Trainable params: 5,474,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "        \"./piano_weights/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\",\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    ")\n",
    "\n",
    "model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"musicgen_weight.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "pattern = network_input[start]\n",
    "prediction_output = []\n",
    "\n",
    "for note_index in range(500):\n",
    "    prediction_input = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "\n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "    index = np.random.choice(n_vocab, 1, p=prediction[0,:])[0]\n",
    "    \n",
    "    prediction_output.append(int_to_note[index])\n",
    "\n",
    "    pattern = np.append(pattern, index)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create song file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove lot of repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-262d814a8bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "pit = iter(prediction_output)\n",
    "for c in pit:\n",
    "    x = c\n",
    "    same = []\n",
    "    while c==x:\n",
    "        same.append(c)\n",
    "        c = next(pit)\n",
    "    if len(same)>2:\n",
    "        processed.extend(same[0:2])\n",
    "    else:\n",
    "        processed.extend(same)\n",
    "    processed.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.mid'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = 0\n",
    "output_notes = []\n",
    "\n",
    "for pattern in prediction_output:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5\n",
    "\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "midi_stream.write('midi', fp=\"test.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is based on this <a href=\"https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\">article</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
